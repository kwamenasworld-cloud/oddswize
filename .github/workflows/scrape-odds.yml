name: Scrape Odds Data

on:
  # Run hourly; the job itself loops every 4 minutes.
  schedule:
    - cron: '0 * * * *'

  # Allow manual trigger
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 70
    concurrency:
      group: odds-scrape
      cancel-in-progress: false
    permissions:
      contents: write
    env:
      ODDS_FAST: 0
      SCRAPE_REPEAT: 14
      SCRAPE_INTERVAL_SECONDS: 240
      REQUIRE_FULL_TOP_LEAGUE_COVERAGE: 0
      ALLOW_SINGLE_BOOKIE_MAJORS: 1

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m playwright install chromium
          # Install Xvfb for running non-headless browsers
          sudo apt-get update
          sudo apt-get install -y xvfb

      - name: Prepare data worktree
        run: |
          git fetch origin data || true
          git worktree add -B data /tmp/odds-data origin/data || git worktree add -B data /tmp/odds-data

      - name: Run odds scraper loop
        env:
          # Cloudflare worker env for odds upload
          CLOUDFLARE_WORKER_URL: ${{ secrets.CLOUDFLARE_WORKER_URL }}
          CLOUDFLARE_API_KEY: ${{ secrets.CLOUDFLARE_API_KEY }}
          D1_CANONICAL_INGEST: ${{ secrets.D1_CANONICAL_INGEST }}
          DISPLAY: ':99'
        run: |
          set -u
          # Start Xvfb (virtual display) in background
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 2

          repeat="${SCRAPE_REPEAT:-1}"
          interval="${SCRAPE_INTERVAL_SECONDS:-240}"
          for i in $(seq 1 "$repeat"); do
            start_ts=$(date +%s)
            echo "Scrape cycle $i/$repeat"
            # Remove any stale snapshot so a failed scrape cannot be re-published
            rm -f odds_data.json
            # Run scraper with virtual display (don't abort whole job on a single failure)
            if ! python scrape_odds_github.py; then
              echo "Scrape failed; skipping this cycle."
              continue
            fi

            if ! python -c $'import json\nfrom datetime import datetime, timezone\nfrom pathlib import Path\npath = Path(\"odds_data.json\")\nif not path.exists():\n    raise SystemExit(\"odds_data.json missing after scrape\")\npayload = json.loads(path.read_text(encoding=\"utf-8\"))\nlast_updated = payload.get(\"last_updated\")\nif not last_updated:\n    raise SystemExit(\"odds_data.json missing last_updated\")\ntry:\n    ts = datetime.fromisoformat(last_updated.replace(\"Z\", \"+00:00\"))\nexcept Exception as exc:\n    raise SystemExit(f\"Invalid last_updated format: {last_updated}\") from exc\nif ts.tzinfo is None:\n    ts = ts.replace(tzinfo=timezone.utc)\nage_min = (datetime.now(timezone.utc) - ts).total_seconds() / 60.0\nprint(f\"Snapshot last_updated: {last_updated} (age {age_min:.1f} min)\")\nif age_min > 20:\n    raise SystemExit(f\"Snapshot too old: {age_min:.1f} min\")'; then
              echo "Snapshot validation failed; skipping this cycle."
              continue
            fi

            git -C /tmp/odds-data rm -rf . || true
            cp odds_data.json /tmp/odds-data/odds_data.json
            git -C /tmp/odds-data add odds_data.json
            git -C /tmp/odds-data -c user.name="oddswize-bot" -c user.email="actions@github.com" commit -m "Update odds data" || true
            git -C /tmp/odds-data push -f origin data

            if [ "$i" -lt "$repeat" ]; then
              end_ts=$(date +%s)
              elapsed=$((end_ts - start_ts))
              sleep_for=$((interval - elapsed))
              if [ "$sleep_for" -gt 0 ]; then
                echo "Sleeping ${sleep_for}s before next cycle"
                sleep "$sleep_for"
              fi
            fi
          done

      - name: Upload results as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: odds-data-${{ github.run_number }}
          path: odds_data.json
          retention-days: 1
