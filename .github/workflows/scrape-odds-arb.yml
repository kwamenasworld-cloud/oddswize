name: Scrape Odds Data (Arbitrage Coverage)

on:
  # Run every 8 minutes with 4-minute in-job cadence.
  schedule:
    - cron: '*/8 * * * *'

  # Allow manual trigger
  workflow_dispatch:

jobs:
  scrape:
    runs-on: [self-hosted, Windows, X64, oddswize]
    timeout-minutes: 15
    concurrency:
      group: odds-scrape-arb-${{ github.ref }}
      cancel-in-progress: false
    permissions:
      contents: write
    env:
      ODDS_FAST: 0
      SCRAPE_REPEAT: 2
      SCRAPE_INTERVAL_SECONDS: 240
      REQUIRE_FULL_TOP_LEAGUE_COVERAGE: 0
      ODDSAPI_SOCCER_LIMIT: 12
      ODDSAPI_SOCCER_KEYS: soccer_epl,soccer_spain_la_liga,soccer_italy_serie_a,soccer_germany_bundesliga,soccer_france_ligue_one,soccer_uefa_champs_league,soccer_uefa_europa_conference_league,soccer_portugal_primeira_liga,soccer_mexico_ligamx,soccer_england_efl_cup
      MAX_MATCHES: 4500
      MAX_CHAMPIONSHIPS: 360
      ALLOW_SINGLE_BOOKIE_MAJORS: 1
      BETWAY_PROXY_URL: ${{ secrets.CLOUDFLARE_WORKER_URL }}
      WORKER_API_SECRET: ${{ secrets.WORKER_API_SECRET }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m playwright install chromium

      - name: Prepare data worktree
        shell: pwsh
        run: |
          $worktreePath = Join-Path $env:TEMP "odds-data-arb"
          try { git fetch origin data-arb | Out-Null } catch {}
          if (Test-Path $worktreePath) {
            try { git worktree remove -f $worktreePath | Out-Null } catch {}
            Remove-Item -Recurse -Force $worktreePath -ErrorAction SilentlyContinue
          }
          git worktree add -B data-arb $worktreePath origin/data-arb
          if ($LASTEXITCODE -ne 0) {
            git worktree add -B data-arb $worktreePath
          }
          "WORKTREE_PATH=$worktreePath" | Out-File -FilePath $env:GITHUB_ENV -Append

      - name: Run odds scraper loop
        env:
          # Cloudflare worker env for odds upload (skipped via --no-push)
          CLOUDFLARE_WORKER_URL: ${{ secrets.CLOUDFLARE_WORKER_URL }}
          CLOUDFLARE_API_KEY: ${{ secrets.CLOUDFLARE_API_KEY }}
          D1_CANONICAL_INGEST: ${{ secrets.D1_CANONICAL_INGEST }}
          ODDSAPI_KEY: ${{ secrets.ODDSAPI_KEY }}
          WORKER_API_SECRET: ${{ secrets.WORKER_API_SECRET }}
        shell: pwsh
        run: |
          $repeat = if ($env:SCRAPE_REPEAT) { [int]$env:SCRAPE_REPEAT } else { 1 }
          $interval = if ($env:SCRAPE_INTERVAL_SECONDS) { [int]$env:SCRAPE_INTERVAL_SECONDS } else { 240 }
          $success = 0

          for ($i = 1; $i -le $repeat; $i++) {
            $start = Get-Date
            Write-Host "Scrape cycle $i/$repeat"
            Remove-Item -ErrorAction SilentlyContinue odds_data.json

            & python scrape_odds_github.py --no-push
            if ($LASTEXITCODE -ne 0) {
              Write-Host "Scrape failed; skipping this cycle."
              continue
            }

            $py = @'
import json
from datetime import datetime, timezone
from pathlib import Path
path = Path("odds_data.json")
if not path.exists():
    raise SystemExit("odds_data.json missing after scrape")
payload = json.loads(path.read_text(encoding="utf-8"))
last_updated = payload.get("last_updated")
if not last_updated:
    raise SystemExit("odds_data.json missing last_updated")
try:
    ts = datetime.fromisoformat(last_updated.replace("Z", "+00:00"))
except Exception as exc:
    raise SystemExit(f"Invalid last_updated format: {last_updated}") from exc
if ts.tzinfo is None:
    ts = ts.replace(tzinfo=timezone.utc)
age_min = (datetime.now(timezone.utc) - ts).total_seconds() / 60.0
print(f"Snapshot last_updated: {last_updated} (age {age_min:.1f} min)")
if age_min > 20:
    raise SystemExit(f"Snapshot too old: {age_min:.1f} min")
'@
            python -c $py
            if ($LASTEXITCODE -ne 0) {
              Write-Host "Snapshot validation failed; skipping this cycle."
              continue
            }
            $success = 1

            $worktree = if ($env:WORKTREE_PATH) { $env:WORKTREE_PATH } else { Join-Path $env:TEMP "odds-data-arb" }
            git -C $worktree reset --hard | Out-Null
            git -C $worktree clean -fdx | Out-Null
            Copy-Item odds_data.json (Join-Path $worktree "odds_data.json") -Force
            if (Test-Path odds_heartbeat.json) {
              Copy-Item odds_heartbeat.json (Join-Path $worktree "odds_heartbeat.json") -Force
            }
            git -C $worktree add odds_data.json
            if (Test-Path (Join-Path $worktree "odds_heartbeat.json")) {
              git -C $worktree add odds_heartbeat.json
            }
            git -C $worktree -c user.name="oddswize-bot" -c user.email="actions@github.com" commit -m "Update arb odds data" | Out-Null
            git -C $worktree push -f origin data-arb

            if ($i -lt $repeat) {
              $elapsed = (Get-Date) - $start
              $sleepFor = $interval - [int]$elapsed.TotalSeconds
              if ($sleepFor -gt 0) {
                Write-Host "Sleeping $sleepFor s before next cycle"
                Start-Sleep -Seconds $sleepFor
              }
            }
          }

          if ($success -eq 0) {
            Write-Error "No successful scrape cycles; failing job."
            exit 1
          }

      - name: Upload results as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: odds-data-arb-${{ github.run_number }}
          path: |
            odds_data.json
            odds_heartbeat.json
          retention-days: 1
